# 📖 Story Generator Documentation

## ✨ Overview

The Story Generator system is an innovative **choose-your-own-adventure story creation engine** powered by Large Language Models (LLMs). This system automatically generates interactive, branching narratives where users make choices that determine their path through the story.

### What it does:

- **🤖 LLM-Powered Creation**: Uses OpenAI's GPT models to generate complete branching stories with multiple paths and endings
- **🌳 Tree Structure**: Creates stories as a tree of interconnected nodes, where each node represents a scene with choices leading to other scenes
- **📊 Structured Output**: Converts LLM-generated text into validated, structured JSON data using Pydantic models
- **💾 Persistent Storage**: Saves the entire story structure to a database using SQLAlchemy for later retrieval and gameplay
- **🎯 Theme-Based**: Generates stories based on user-specified themes (fantasy, sci-fi, mystery, etc.)

### Key Technologies:

- **🦜 LangChain**: Manages LLM interactions, prompt templates, and output parsing
- **📋 Pydantic**: Validates and structures the LLM output into strongly-typed Python objects
- **🗄️ SQLAlchemy**: Handles database operations and relationships for story persistence
- **⚡ FastAPI**: Provides the web API endpoints (used by the broader application)

---

## 🧱 Core Models: `core/models.py`

These Pydantic models define the exact structure that the LLM must follow when generating stories. They serve as both **validation schemas** and **documentation** for the AI.

### `StoryOptionLLM`

Represents a single choice that a user can make at any point in the story.

```python
class StoryOptionLLM(BaseModel):
    text: str = Field(description="the text of the option shown to the user")
    nextNode: Dict[str, Any] = Field(description="the next node content and its options")
```

**Fields:**
- **`text`**: The actual choice text displayed to the user (e.g., "Enter the dark cave", "Talk to the merchant")
- **`nextNode`**: A complete `StoryNodeLLM` object representing what happens when this choice is selected

### `StoryNodeLLM`

Represents a single scene or "moment" in the story tree.

```python
class StoryNodeLLM(BaseModel):
    content: str = Field(description="The main content of the story node")
    isEnding: bool = Field(description="Whether this node is an ending node")
    isWinningEnding: bool = Field(description="Whether this node is a winning ending node")
    options: Optional[List[StoryOptionLLM]] = Field(default=None, description="The options for this node")
```

**Fields:**
- **`content`**: The narrative text for this scene (e.g., "You find yourself at a crossroads in the enchanted forest...")
- **`isEnding`**: `True` if this node concludes the story (no more choices)
- **`isWinningEnding`**: `True` if this ending represents success/victory for the player
- **`options`**: List of choices available to the user (empty/null for ending nodes)

### `StoryLLMResponse`

The top-level container for the complete story generated by the LLM.

```python
class StoryLLMResponse(BaseModel):
    title: str = Field(description="The title of the story")
    rootNode: StoryNodeLLM = Field(description="The root node of the story")
```

**Fields:**
- **`title`**: The story's title (e.g., "The Enchanted Forest Quest")
- **`rootNode`**: The starting point of the story tree

### ✅ Why We Use `Field(description=...)` 

The `description` parameters aren't just documentation—they're **AI guidance tools**:

1. **🎯 LLM Understanding**: When LangChain's `PydanticOutputParser` generates format instructions, these descriptions are included to help the LLM understand exactly what each field should contain.

2. **📝 Schema Documentation**: The descriptions become part of the JSON schema that's sent to the LLM, acting as inline documentation.

3. **🎨 Output Quality**: More descriptive field definitions lead to more accurate and contextually appropriate LLM outputs.

**Example**: Without descriptions, an LLM might put a story node's content in the wrong field. With clear descriptions, it knows that `content` should contain narrative text, not metadata.

---

## 🧠 The LLM Prompt: `core/prompts.py`

The `STORY_PROMPT` is the **instruction manual** that tells the LLM exactly how to create stories.

### Key Components:

```python
STORY_PROMPT = """
You are a creative story writer that creates engaging choose-your-own-adventure stories.
Generate a complete branching story with multiple paths and endings in the JSON format I'll specify.

The story should have:
1. A compelling title
2. A starting situation (root node) with 2-3 options
3. Each option should lead to another node with its own options
4. Some paths should lead to endings (both winning and losing)
5. At least one path should lead to a winning ending

Story structure requirements:
- Each node should have 2-3 options except for ending nodes
- The story should be 3-4 levels deep (including root node)
- Add variety in the path lengths (some end earlier, some later)
- Make sure there's at least one winning path

Output your story in this exact JSON structure:
{format_instructions}
"""
```

### How It Works:

1. **🎭 Role Definition**: "You are a creative story writer..." establishes the LLM's persona
2. **📋 Structural Requirements**: Specifies exactly how many options, depth levels, and ending types
3. **🎯 Quality Guidelines**: Ensures engaging content with varied path lengths
4. **📄 Format Instructions**: The `{format_instructions}` placeholder gets filled by LangChain with the Pydantic schema

### The `{format_instructions}` Magic:

When LangChain processes this prompt, it replaces `{format_instructions}` with a detailed JSON schema generated from our Pydantic models. This gives the LLM a precise template to follow, including:
- Required fields and their types
- Field descriptions for context
- Nested structure requirements
- Validation rules

**Example of what gets injected:**
```json
{
  "type": "object",
  "properties": {
    "title": {"type": "string", "description": "The title of the story"},
    "rootNode": {
      "type": "object",
      "properties": {
        "content": {"type": "string", "description": "The main content of the story node"},
        // ... more schema details
      }
    }
  }
}
```

---

## 🏗️ Story Generation Pipeline: `StoryGenerator` Class

The `StoryGenerator` class orchestrates the entire story creation process. It's organized as a class to:

- **🔧 Reuse Components**: Share the LLM configuration across methods
- **📦 Organize Logic**: Group related functionality together
- **🧪 Enable Testing**: Make it easy to mock and test individual components
- **🔄 Support Future Extensions**: Easy to add new generation methods or configurations

### 🔐 `_get_llm(cls)` - LLM Configuration

```python
@classmethod
def _get_llm(cls):
    openai_api_key = os.getenv("CHOREO_OPENAI_CONNECTION_OPENAI_API_KEY")
    serviceurl = os.getenv("CHOREO_OPENAI_CONNECTION_SERVICEURL")

    if openai_api_key and serviceurl:
        return ChatOpenAI(model="gpt-4o-mini", api_key=openai_api_key, base_url=serviceurl)

    return ChatOpenAI(model="gpt-4o-mini")
```

**Purpose**: Creates and configures the OpenAI LLM instance for story generation.

**Why the `_` prefix?**
- Python convention for "internal" or "private" methods
- Indicates this method is for internal class use, not external API calls

**Why `@classmethod`?**
- Can be called on the class itself without creating an instance
- Allows sharing the LLM configuration across all story generation methods
- Provides `cls` parameter to access class-level information

**Configuration Logic**:
1. **🔑 Environment Variables**: Tries to load custom OpenAI credentials (useful for enterprise setups)
2. **🌐 Custom Service URL**: Supports custom OpenAI-compatible endpoints (useful for Azure OpenAI, etc.)
3. **🛡️ Fallback**: Uses default OpenAI configuration if custom settings aren't available

---

### 🛠️ `generate_story(cls, db: Session, session_id: str, theme: str)` - Main Generation Pipeline

This is the **heart of the system**—it orchestrates the entire story creation process.

```python
@classmethod
def generate_story(cls, db: Session, session_id: str, theme: str = "fantasy") -> Story:
```

Let's break down each step:

#### Step 1: Initialize Components
```python
llm = cls._get_llm()
story_parser = PydanticOutputParser(pydantic_object=StoryLLMResponse)
```

- **`llm`**: Gets the configured OpenAI instance
- **`story_parser`**: Creates a LangChain parser that knows how to convert LLM text output into `StoryLLMResponse` objects

**What is `PydanticOutputParser`?**
- A LangChain component that automatically generates format instructions from Pydantic models
- Validates LLM output against the schema
- Handles parsing errors gracefully
- Converts JSON strings into strongly-typed Python objects

#### Step 2: Build the Prompt
```python
prompt = ChatPromptTemplate.from_messages([
    ("system", STORY_PROMPT),
    ("human", f"Create the story with this theme: {theme}")
]).partial(format_instructions=story_parser.get_format_instructions())
```

- **`ChatPromptTemplate.from_messages()`**: Creates a conversation-style prompt with system and human messages
- **`"system"`**: The AI's instructions (our `STORY_PROMPT`)
- **`"human"`**: The user's request with the specific theme
- **`.partial(format_instructions=...)`**: Injects the Pydantic schema into the `{format_instructions}` placeholder

**What is `.partial()`?**
- A LangChain method that "pre-fills" template variables
- Allows us to inject the format instructions once and reuse the prompt
- More efficient than formatting the entire prompt each time

#### Step 3: Call the LLM
```python
raw_response = llm.invoke(prompt.invoke({}))
```

- **`prompt.invoke({})`**: Formats the final prompt (empty dict because we used `.partial()`)
- **`llm.invoke()`**: Sends the prompt to OpenAI and gets the response

#### Step 4: Handle Response Variations
```python
response_text = raw_response
if hasattr(raw_response, "content"):
    response_text = raw_response.content
```

**What is `hasattr()`?**
- Checks if an object has a specific attribute
- Needed because different LangChain versions return responses in different formats
- Some return strings directly, others return objects with a `.content` attribute

#### Step 5: Parse the LLM Output
```python
story_structure = story_parser.parse(response_text)
```

**What happens at `.parse()`?**
- Validates the JSON against our Pydantic schema
- Converts the text into a `StoryLLMResponse` object
- Raises validation errors if the structure is incorrect
- Handles type conversions automatically

#### Step 6: Create Database Record
```python
story_db = Story(title=story_structure.title, session_id=session_id)
db.add(story_db)
db.flush()
```

- **`Story()`**: Creates a SQLAlchemy model instance
- **`db.add()`**: Stages the record for insertion
- **`db.flush()`**: Immediately executes the INSERT to get the auto-generated ID

**What is `db.flush()`?**
- Executes pending SQL operations without committing the transaction
- Essential here because we need the `story_db.id` for creating related `StoryNode` records
- Different from `commit()` because it doesn't end the transaction

#### Step 7: Handle Model Validation
```python
root_node_data = story_structure.rootNode
if isinstance(root_node_data, dict):
    root_node_data = StoryNodeLLM.model_validate(root_node_data)
```

**What is `isinstance()`?**
- Checks if an object is of a specific type
- Needed because sometimes Pydantic returns dicts instead of model instances

**What is `model_validate()`?**
- Pydantic method that converts dictionaries into model instances
- Performs full validation during conversion
- Ensures we have strongly-typed objects for the next step

#### Step 8: Process the Story Tree
```python
cls._process_story_node(db, story_db.id, root_node_data, is_root=True)
```

- Recursively processes the entire story tree
- Saves each node and its relationships to the database

#### Step 9: Commit and Return
```python
db.commit()
return story_db
```

- **`db.commit()`**: Permanently saves all changes to the database
- Returns the complete `Story` object for further use

---

### 🔁 `_process_story_node(...)` - Recursive Tree Builder

This function recursively walks through the story tree and saves each node to the database.

```python
@classmethod
def _process_story_node(cls, db: Session, story_id: int, node_data: StoryNodeLLM, is_root: bool = False) -> StoryNode:
```

**Why is this function recursive?**
- Stories are tree structures with potentially unlimited depth
- Each option leads to another node, which may have its own options
- Recursion naturally handles this nested structure

#### Step 1: Create the Database Node
```python
node = StoryNode(
    story_id=story_id,
    content=node_data.content if hasattr(node_data, "content") else node_data["content"],
    is_root=is_root,
    is_ending=node_data.isEnding if hasattr(node_data, "isEnding") else node_data["isEnding"],
    is_winning_ending=node_data.isWinningEnding if hasattr(node_data, "isWinningEnding") else node_data["isWinningEnding"],
    options=[]
)
db.add(node)
db.flush()
```

- **Defensive Programming**: Uses `hasattr()` to handle both Pydantic models and dictionaries
- **`options=[]`**: Starts with empty options; they'll be populated later
- **`db.flush()`**: Gets the database ID immediately for linking child nodes

#### Step 2: Process Child Nodes (if not an ending)
```python
if not node.is_ending and (hasattr(node_data, "options") and node_data.options):
    options_list = []
    for option_data in node_data.options:
        next_node = option_data.nextNode

        if isinstance(next_node, dict):
            next_node = StoryNodeLLM.model_validate(next_node)

        child_node = cls._process_story_node(db, story_id, next_node, False)

        options_list.append({
            "text": option_data.text,
            "node_id": child_node.id
        })

    node.options = options_list
```

**What happens in this loop?**

1. **🔍 Extract Next Node**: Gets the `nextNode` from each option
2. **✅ Validate Data**: Converts dicts to Pydantic models if needed
3. **🔄 Recurse**: Calls itself to process the child node
4. **🔗 Link Nodes**: Creates option objects that reference the child node by ID
5. **💾 Store Options**: Saves the options as JSON in the current node

**Why store options as JSON?**
- Simpler than creating a separate `Option` table
- Options are always loaded with their parent node
- Reduces database complexity for this use case

#### Step 3: Final Save
```python
db.flush()
return node
```

- Saves the updated node with its options
- Returns the node for linking by parent calls

---

## 📊 Database Models (`models/story_model.py`)

The SQLAlchemy models define how stories are stored in the database.

### `Story` - Main Story Container

```python
class Story(Base):
    __tablename__ = "stories"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    session_id = Column(String, index=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    nodes = relationship("StoryNode", back_populates="story")
```

**Fields:**
- **`id`**: Auto-incrementing primary key
- **`title`**: Story title from the LLM
- **`session_id`**: Links the story to a user session
- **`created_at`**: Automatic timestamp
- **`nodes`**: Relationship to all story nodes

### `StoryNode` - Individual Story Scenes

```python
class StoryNode(Base):
    __tablename__ = "story_nodes"

    id = Column(Integer, primary_key=True, index=True)
    story_id = Column(Integer, ForeignKey("stories.id"), index=True)
    content = Column(String, index=True)
    is_root = Column(Boolean, default=False)
    is_ending = Column(Boolean, default=False)
    is_winning_ending = Column(Boolean, default=False)
    options = Column(JSON, default=list)

    story = relationship("Story", back_populates="nodes")
```

**Fields:**
- **`story_id`**: Foreign key linking to the parent story
- **`content`**: The narrative text for this scene
- **`is_root`**: True for the starting node
- **`is_ending`**: True for terminal nodes
- **`is_winning_ending`**: True for successful endings
- **`options`**: JSON array of choices and their target node IDs

**Why use JSON for options?**
- Options are always loaded with their parent node
- Simpler schema than separate tables
- Efficient for read-heavy operations

---

## 🧩 Integration Summary

Here's how all the pieces work together:

1. **📝 User Request**: API receives a request for a story with a specific theme
2. **🤖 LLM Call**: System sends structured prompt to OpenAI
3. **📄 JSON Response**: LLM returns a complete story in JSON format
4. **✅ Validation**: Pydantic validates the structure and converts to Python objects
5. **🗄️ Database Storage**: Recursive function saves the tree structure to SQL database
6. **🎮 Ready for Use**: Story is available for interactive gameplay

### The Data Flow:

```
Theme Input → LLM Prompt → OpenAI API → JSON Response → Pydantic Validation → SQLAlchemy Models → Database Storage
```

### Why This Architecture?

- **🔒 Type Safety**: Pydantic ensures LLM output matches our expectations
- **🧩 Modularity**: Each component has a single responsibility
- **🔄 Reusability**: Components can be used in different contexts
- **🧪 Testability**: Each piece can be tested independently
- **📈 Scalability**: Database design supports efficient queries and updates

---

## 📚 Example Story Output

Here's what a typical LLM response looks like:

```json
{
  "title": "The Enchanted Forest Quest",
  "rootNode": {
    "content": "You stand at the edge of an ancient forest, mysterious mists swirling between the trees. A worn path leads deeper into the woods, while a small cottage with smoke rising from its chimney sits nearby.",
    "isEnding": false,
    "isWinningEnding": false,
    "options": [
      {
        "text": "Follow the path into the forest",
        "nextNode": {
          "content": "The path leads to a clearing where a crystal fountain bubbles with magical water.",
          "isEnding": false,
          "isWinningEnding": false,
          "options": [
            {
              "text": "Drink from the fountain",
              "nextNode": {
                "content": "The magical water grants you wisdom and strength. You emerge as the forest's new guardian!",
                "isEnding": true,
                "isWinningEnding": true,
                "options": null
              }
            }
          ]
        }
      },
      {
        "text": "Approach the cottage",
        "nextNode": {
          "content": "An old witch greets you with a suspicious smile...",
          "isEnding": false,
          "isWinningEnding": false,
          "options": [...]
        }
      }
    ]
  }
}
```

This structure maps perfectly to our Pydantic models and creates an engaging branching narrative.

---

## 🔍 Glossary

### **`PydanticOutputParser`**
A LangChain component that automatically converts LLM text responses into validated Pydantic objects. It generates format instructions and handles parsing errors.

### **`ChatPromptTemplate`**
LangChain's system for creating conversation-style prompts with system messages (instructions) and human messages (user input).

### **`db.flush()`**
SQLAlchemy method that executes pending database operations without committing the transaction. Essential for getting auto-generated IDs while keeping the transaction open.

### **`@classmethod`**
Python decorator that makes a method belong to the class rather than instances. The method receives `cls` (the class) as its first parameter instead of `self`.

### **`model_validate()`**
Pydantic method that converts dictionaries or other data into validated model instances, performing full type checking and validation.

### **`.partial()`**
LangChain method for pre-filling template variables. Allows injecting some variables early while leaving others for later completion.

### **`hasattr()`**
Python built-in function that checks if an object has a specific attribute. Used for defensive programming when object structure might vary.

### **LangChain**
A framework for building applications with large language models. Provides tools for prompt management, output parsing, and workflow orchestration.

### **OpenAI API Key**
Authentication credential for accessing OpenAI's language models. Can be configured for different environments or custom endpoints.

### **JSON Schema**
A standard for describing JSON data structures. Pydantic automatically generates schemas that LLMs can use to understand required output formats.

### **Tree Structure**
A hierarchical data organization where each node can have multiple children but only one parent. Perfect for representing branching narratives.

### **Foreign Key**
A database concept where one table references the primary key of another table, creating relationships between data.

---

**🎉 Congratulations!** You now understand how this sophisticated LLM-powered story generation system works. The combination of LangChain's prompt management, Pydantic's data validation, and SQLAlchemy's database handling creates a robust pipeline for generating and storing interactive narratives. 